# ğŸš€ Fine-Tuning GPT-2

This repository contains the code and configurations for fine-tuning **GPT-2** on a custom dataset. The model has been trained using **Hugging Face's Transformers** library, leveraging PyTorch for deep learning optimizations.

## ğŸ“‚ Project Structure

```
ğŸ“¦ Fine-Tuning-GPT2
â”‚â”€â”€ ğŸ“ checkpoint-15500     # Latest model checkpoint
â”‚â”€â”€ ğŸ“ runs                 # Training logs and tensorboard outputs
â”‚â”€â”€ ğŸ“œ config.json          # Model configuration file
â”‚â”€â”€ ğŸ“œ generation_config.json  # Configuration for text generation
â”‚â”€â”€ ğŸ“œ merges.txt           # Tokenizer merge file
â”‚â”€â”€ ğŸ“œ special_tokens_map.json  # Special tokens configuration
â”‚â”€â”€ ğŸ“œ tokenizer_config.json   # Tokenizer settings
â”‚â”€â”€ ğŸ“œ training_args.bin    # Training arguments and hyperparameters
â”‚â”€â”€ ğŸ“œ vocab.json           # Tokenizer vocabulary file
â”‚â”€â”€ ğŸ“œ GPT2.ipynb           # Jupyter Notebook for fine-tuning and inference
```

## ğŸ“Œ Features

âœ… Fine-tunes **GPT-2** on a custom dataset  
âœ… Utilizes **Hugging Face Transformers** for model training  
âœ… Includes model checkpoints for continued training  
âœ… Supports **special tokens** and custom vocabulary  
âœ… Implements **text generation** using the trained model  

## ğŸ›  Installation

Make sure you have Python 3.8+ installed. Then, install the required dependencies:

```bash
pip install torch transformers datasets safetensors
```

## ğŸš€ Fine-Tuning GPT-2

You can fine-tune the model using the provided Jupyter Notebook:

```bash
jupyter notebook GPT2.ipynb
```

Alternatively, you can run the training script in a Python environment:

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Custom training arguments
training_args = TrainingArguments(
    output_dir="./checkpoint",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=500,
    save_total_limit=2
)

# Define trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=your_dataset
)

# Start training
trainer.train()
```

## ğŸ“Š Generating Text with Fine-Tuned Model

Once the model is trained, you can use it to generate text:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="./checkpoint-15500")
output = generator("Once upon a time", max_length=50)
print(output)
```

## ğŸ“ˆ Training Logs & Checkpoints

- Model checkpoints are stored in the `checkpoint-15500/` directory.
- Training logs are available in the `runs/` folder for TensorBoard visualization.

## ğŸ“„ License

This project is licensed under the **MIT License**.

---

ğŸ“§ **Contact:** ali.abdien.omar@gmail.com  

---
